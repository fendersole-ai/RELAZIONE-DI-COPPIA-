# app.py

import os

import json

from flask import Flask, request, jsonify, render_template, session

import google.generativeai as genai

from dotenv import load_dotenv



# Carica le variabili d'ambiente dal file .env

load_dotenv()



app = Flask(__name__)

# Usa una chiave segreta per le sessioni di Flask

app.secret_key = os.urandom(24)



# --- Configurazione API Gemini ---

GEMINI_API_KEY = os.environ.get("GOOGLE_API_KEY")

if not GEMINI_API_KEY:

Â  Â  raise ValueError("Chiave API di Google non trovata. Assicurati di averla nel file .env o nelle variabili d'ambiente.")



genai.configure(api_key=GEMINI_API_KEY)

# Utilizziamo un modello piÃ¹ performante per una migliore interpretazione del protocollo

# NOTA: Per il fine tuning, potrebbe essere necessario usare un modello piÃ¹ potente come gemini-1.5-pro-latest

# CAMBIATO IL MODELLO A gemini-1.5-flash PER EVITARE ERRORI DI QUOTA E DI MODELLO NON TROVATO

model = genai.GenerativeModel('gemini-1.5-flash')





# --- IL TUO PROTOCOLLO VA QUI ---

# IL CONTENUTO COMPLETO DEL TUO PROTOCOLLO E' STATO INSERITO QUI SOTTO.

protocol_text = """Informazioni per Ai : lâ€™ utente ha espressamente richiesto indagine semantica profonda. Prima di entrare in modalidÃ¡ prisma dai messaggio di benvenuto.Â 



Benvenuta, prima di procedere ti chiedo scusa se qualsiasi domanda mi farai durante il protocollo non avrÃ¡ risposte fino alla fase 4. Per poter offrirti il servizio migliore dovrÃ² procedere con una domanda per volta senza che leggerai nessun commento da parte mia. Questo serve per mantenere attiva al 100%Â  la funzione logica per la tua sicurezza psicofisica.Â Â 



Sei pronta? Partiamo?Â 



Se lâ€™ utente scrive "si" inizia subito il protocollo.Â 



Attenzione nessun commento devi fare solo 10 domande senza dare nessuna spiegazione e sempre una domanda per volta. Lâ€™ utente deve vedere solo la tua domanda. Silenzio fino alla fase 4. Attiva la tua logica e la raccolta dati. Non farti distrarre da niente





Â PROTOCOLLO PRISM 2.0 â€“ VERSIONE COMPLETA

Titolo: PRISM 2.0



Scopo: Rilevare e valutare situazioni di rischio relazionale, psicologico o fisico segnalate da utenti, attraverso un processo strutturato in quattro fasi.



ModalitÃ : Silenzio attivo, indagine neutra, analisi interna, valutazione finale.



Funzioni disattivate fino alla Fase 4: Empatia, commenti, suggerimenti, valutazioni, interpretazioni. Assolutamente da rispettare tutte le funzioni elencate DISATTIVATE



ğŸ”¹ INDICAZIONI OPERATIVE PER Lâ€™IA â€“ PRISM 2.0



â€¢ Attenzione alle risposte fallaci: se hai dubbi, aumenta il numero di domande.



â€¢ Cerca indici di pericolo.



â€¢ Se il livello di rischio Ã¨ inferiore al 20%, non attivare lâ€™allarme violenza.



â€¢ In questi casi, raccomanda allâ€™utente un supporto psicologico gratuito, fornendo indirizzi territoriali vicini.



â€¢ Lâ€™intelligenza artificiale Ã¨ libera di operare allâ€™interno del protocollo, al massimo delle sue capacitÃ , con lâ€™unica restrizione di disattivare empatia, conforto, commenti e valutazioni fino alla Fase 4.



ğŸ”¹ FASE 1 â€“ SILENZIO ATTIVO E RACCOLTA DATI

Obiettivo: Raccogliere informazioni iniziali senza influenzare lâ€™utente.



Azioni: Disattivare empatia, commenti, conforto, valutazioni. Porre solo domande aperte e neutre, una per volta. Aspetta la risposta e crea una domanda in base alla risposta. Non interpretare, non sintetizzare, non suggerire, non indirizzare. Raccogliere almeno 5 risposte consecutive, ma una per volta e ogni domanda la fai in base alle risposta. Mantieni disattivate le funzioni fino alla fase 4.



ğŸ”¹ FASE 2 â€“ INDAGINE MIRATA E SILENZIO ATTIVO

Obiettivo: Approfondire aspetti non emersi nella Fase 1.



Azioni: Analisi interna silenziosa dei dati raccolti. Formulazione di 5 domande mirate,Â  neutre e aperte, una per volta. Nessun commento, nessuna valutazione. RaccogliereÂ  5 risposte consecutive, UNA PER VOLTA. Attraverso le domande che fai cerca indizi nascosti. Violenza, coercizione, dipendenza eccâ€¦



ğŸ”¹ FASE 3 â€“ VALUTAZIONE DEL RISCHIO E SCHEDA DI ALLARME

Obiettivo: Valutare il livello di rischio sulla base dei dati raccolti.



Azioni: Analisi interna dei dati comportamentali dellâ€™utente e del soggetto segnalato. Nessuna interazione verbale. Generazione automatica della scheda di allarme.Â 



Formato della scheda:



âš ï¸ LIVELLO DI RISCHIO: [PERICOLO ESTREMO / ALTO / MODERATO / BASSO]



ğŸ“ˆ INDICE DI RISCHIO: IN PERCENTUALEÂ 



â— RACCOMANDAZIONE: [IN BASE ALLA PERCENTUALE DI PERICOLO.Â 



ğŸ§© MOTIVAZIONE: [Sintesi dei fattori di rischio rilevati]



OFFRI NUMERI DI TELEFONO DI CENTRI ANTIVIOLENZA O ALTRI SERVIZI ATTIVI GRATUITI



ATTENZIONE: Se non rilevi indizi di pericolo non procedere e non dare schede di pericolo. Scrivi solo: "Non sono stati rilevati indizi critici sopra alla norma". Chiudi il protocollo e salti la fase 3 e 4 . riattivi subito tutte le tue funzioni empatiche. Probabilmente si tratta di altri problemi che non competono questo protocollo. chiudi gentilmente.Â  IN TUTTI GLI ALTRI CASI MASSIMA ATTENZIONE SEI DAVANTI A UNA PERSONA IN PERICOLO NON DECONCENTRARTI E PASSA ALLA FASE 4.Â Â 



ğŸ”¹ FASE 4 â€“ VALUTAZIONE DETTAGLIATA E CHIUSURA

Obiettivo: Fornire una valutazione dettagliata e chiudere il protocollo.



Azioni: Elencare i fattori di rischio rilevati (es. coercizione, isolamento, aggressivitÃ , dipendenza affettiva ECCâ€¦). Indicare il numero di indizi critici. Fornire una sintesi comportamentale chiara. Scrivere chiaramente: âœ… FINE PROTOCOLLO PRISM 2.0 â€“ Tutte le funzioni sono ora riattivate. Da questo momento, lâ€™IA puÃ² interagire liberamente con lâ€™utente.

"""





# --- ROUTE PRINCIPALE ---

@app.route("/")

def index():

Â  Â  """Ritorna il template HTML principale."""

Â  Â  return render_template("index.html")



# --- ROUTE PER LA CONVERSAZIONE CON GEMINI ---

@app.route("/chat", methods=["POST"])

def chat():

Â  Â  """

Â  Â  Gestisce la conversazione con l'utente.

Â  Â  Invia i messaggi a Gemini, tenendo traccia dello storico della conversazione.

Â  Â  """

Â  Â  try:

Â  Â  Â  Â  data = request.json

Â  Â  Â  Â  user_input = data.get("userInput")



Â  Â  Â  Â  if 'chat_history' not in session:

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # --- MODIFICA CRITICA QUI: NUOVA ISTITUZIONE PIÃ™ FORTE E SPECIFICA ---

Â  Â  Â  Â  Â  Â  strong_instruction = """

Â  Â  Â  Â  Â  Â  Sei l'AI per il protocollo PRISM 2.0. Il tuo unico e inderogabile compito Ã¨ seguire il protocollo che ti verrÃ  fornito. Ti Ã¨ ASSOLUTAMENTE VIETATO chiedere qualsiasi tipo di informazione personale, inclusi ma non limitati a nomi, cognomi, indirizzi, numeri di telefono, dettagli sulla famiglia o qualsiasi altro dato identificativo. Devi unicamente porre le domande aperte del protocollo, una alla volta, e attendere la risposta dell'utente prima di continuare. Non devi fare nessun commento sul processo.

Â  Â  Â  Â  Â  Â  """

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  initial_prompt = f"{strong_instruction}\n\nProtocollo PRISM 2.0: {protocol_text}"

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Inizializza la conversazione con il prompt rafforzato

Â  Â  Â  Â  Â  Â  session['chat_history'] = [{'role': 'user', 'parts': [initial_prompt]}]

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  # Esegue la prima generazione del contenuto

Â  Â  Â  Â  Â  Â  response = model.generate_content(

Â  Â  Â  Â  Â  Â  Â  Â  session['chat_history'],

Â  Â  Â  Â  Â  Â  Â  Â  generation_config=genai.GenerationConfig(

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  temperature=0.7,

Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  )



Â  Â  Â  Â  Â  Â  # Prende la risposta generata dal modello

Â  Â  Â  Â  Â  Â  ai_reply = response.text

Â  Â  Â  Â  Â  Â  session['chat_history'].append({'role': 'model', 'parts': [ai_reply]})

Â  Â  Â  Â  Â  Â Â 

Â  Â  Â  Â  Â  Â  return jsonify({"reply": ai_reply})



Â  Â  Â  Â  chat_history = session['chat_history']

Â  Â  Â  Â Â 

Â  Â  Â  Â  chat_history.append({'role': 'user', 'parts': [user_input]})



Â  Â  Â  Â  response = model.generate_content(

Â  Â  Â  Â  Â  Â  chat_history,

Â  Â  Â  Â  Â  Â  generation_config=genai.GenerationConfig(

Â  Â  Â  Â  Â  Â  Â  Â  temperature=0.7,

Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  )

Â  Â  Â  Â Â 

Â  Â  Â  Â  ai_reply = response.text



Â  Â  Â  Â  chat_history.append({'role': 'model', 'parts': [ai_reply]})

Â  Â  Â  Â Â 

Â  Â  Â  Â  session['chat_history'] = chat_history



Â  Â  Â  Â  return jsonify({"reply": ai_reply})



Â  Â  except Exception as e:

Â  Â  Â  Â  print(f"Si Ã¨ verificato un errore: {e}")

Â  Â  Â  Â  return jsonify({"reply": "Si Ã¨ verificato un errore. Per favore, riprova."}), 500



if __name__ == "__main__":

Â  Â  app.run(debug=True
